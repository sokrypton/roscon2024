{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/roscon2024/blob/main/finetune_esm2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**finefine esm2** (tutorial for wednesday)\n",
        "Special Thanks to Amelie Schreiber\n",
        "https://github.com/Amelie-Schreiber/esm2_loras"
      ],
      "metadata": {
        "id": "wqjacq79TckC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"esm2_t6_8M_UR50D\" # @param [\"esm2_t33_650M_UR50D\", \"esm2_t30_150M_UR50D\", \"esm2_t12_35M_UR50D\", \"esm2_t6_8M_UR50D\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k77krszTpJAn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtRKmskxgHfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebdf601-c233-4f74-8f73-e5eb3d66cc75"
      },
      "source": [
        "%%time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import EsmForSequenceClassification, EsmForTokenClassification, AutoTokenizer\n",
        "\n",
        "trainable_params = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n",
        "                                                  num_labels=1,\n",
        "                                                  hidden_dropout_prob=0.15)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.17 s, sys: 356 ms, total: 2.52 s\n",
            "Wall time: 3.93 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8XmamkAjpQL",
        "outputId": "03571086-5ff7-42d4-d98f-abdc770d4726"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7737722"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqa5GLFzJGqv",
        "outputId": "4006fa3c-b2ea-4e4f-ec23-1fbca5018a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EsmForTokenClassification(\n",
              "  (esm): EsmModel(\n",
              "    (embeddings): EsmEmbeddings(\n",
              "      (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
              "    )\n",
              "    (encoder): EsmEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x EsmLayer(\n",
              "          (attention): EsmAttention(\n",
              "            (self): EsmSelfAttention(\n",
              "              (query): Linear(in_features=320, out_features=320, bias=True)\n",
              "              (key): Linear(in_features=320, out_features=320, bias=True)\n",
              "              (value): Linear(in_features=320, out_features=320, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "              (rotary_embeddings): RotaryEmbedding()\n",
              "            )\n",
              "            (output): EsmSelfOutput(\n",
              "              (dense): Linear(in_features=320, out_features=320, bias=True)\n",
              "              (dropout): Dropout(p=0.15, inplace=False)\n",
              "            )\n",
              "            (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (intermediate): EsmIntermediate(\n",
              "            (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
              "          )\n",
              "          (output): EsmOutput(\n",
              "            (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
              "            (dropout): Dropout(p=0.15, inplace=False)\n",
              "          )\n",
              "          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (contact_head): EsmContactPredictionHead(\n",
              "      (regression): Linear(in_features=120, out_features=1, bias=True)\n",
              "      (activation): Sigmoid()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.15, inplace=False)\n",
              "  (classifier): Linear(in_features=320, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Get DATA\n",
        "batch_size = 32 # @param {\"type\":\"integer\"}\n",
        "max_crop_len = 512 # @param {\"type\":\"integer\"}\n",
        "\n",
        "!wget -qnc https://github.com/sokrypton/roscon2024/raw/main/af2bind_data_0.pkl\n",
        "import pickle\n",
        "with open(\"af2bind_data_0.pkl\", \"rb\") as handle:\n",
        "  DATA = pickle.load(handle)\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "# Helper function to pad sequences\n",
        "def pad_sequence(seq, max_len, pad_value=0):\n",
        "    pad_size = max(0, max_len - len(seq))\n",
        "    return np.pad(seq, (0, pad_size), 'constant', constant_values=pad_value)[:max_len]\n",
        "\n",
        "class CustomProteinDataset(Dataset):\n",
        "    def __init__(self, inputs, attention_masks, outputs, masks, max_crop_len=128):\n",
        "        self.inputs = inputs\n",
        "        self.attention_masks = attention_masks\n",
        "        self.outputs = outputs\n",
        "        self.masks = masks\n",
        "        self.max_crop_len = max_crop_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.inputs[idx]\n",
        "        attention_mask = self.attention_masks[idx]\n",
        "        output = self.outputs[idx]\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        # Calculate the true length of the sequence (where attention_mask == 1)\n",
        "        true_len = int(np.sum(attention_mask))\n",
        "\n",
        "        # Determine the crop length (if the true length is less than max_crop_len, use true_len)\n",
        "        crop_len = min(self.max_crop_len, true_len)\n",
        "\n",
        "        # Randomly sample a crop starting index\n",
        "        if true_len > crop_len:\n",
        "            start_idx = np.random.randint(0, true_len - crop_len + 1)\n",
        "        else:\n",
        "            start_idx = 0\n",
        "\n",
        "        # Crop the sequences\n",
        "        input_ids = input_ids[start_idx:start_idx + crop_len]\n",
        "        attention_mask = attention_mask[start_idx:start_idx + crop_len].astype(np.float32)\n",
        "        output = output[start_idx:start_idx + crop_len].astype(np.float32)\n",
        "        mask = mask[start_idx:start_idx + crop_len].astype(np.float32)\n",
        "\n",
        "        # Pad the cropped sequences to max_crop_len\n",
        "        input_ids = pad_sequence(input_ids, self.max_crop_len)\n",
        "        attention_mask = pad_sequence(attention_mask, self.max_crop_len)\n",
        "        output = pad_sequence(output, self.max_crop_len)\n",
        "        mask = pad_sequence(mask, self.max_crop_len)\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(output), torch.tensor(mask)\n",
        "\n",
        "# Create DataLoaders\n",
        "dataloaders = []\n",
        "for v in range(3):  # train/test/validation\n",
        "    dataset = CustomProteinDataset(DATA[\"inputs\"][v], DATA[\"attention_masks\"][v],\n",
        "                                   DATA[\"outputs\"][v], DATA[\"masks\"][v],\n",
        "                                   max_crop_len=max_crop_len)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=(v == 0))\n",
        "    dataloaders.append(dataloader)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fUQAZkhEIRzU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Training Code\n",
        "\n",
        "def compute_loss(logits, labels, mask):\n",
        "  \"\"\"Compute masked loss.\"\"\"\n",
        "  loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n",
        "  masked_loss = loss * mask\n",
        "  mean_loss = masked_loss.sum() / mask.sum()\n",
        "  return mean_loss\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer):\n",
        "  \"\"\"Train the model for one epoch.\"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n",
        "    logits = outputs.logits.squeeze(-1)\n",
        "\n",
        "    # Compute loss\n",
        "    mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    mean_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += mean_loss.item()\n",
        "\n",
        "  average_loss = total_loss / len(dataloader)\n",
        "  return average_loss\n",
        "\n",
        "def validate(model, dataloader):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in dataloader:\n",
        "        inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits.squeeze(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n",
        "\n",
        "        total_loss += mean_loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, num_epochs, optimizer):\n",
        "    \"\"\"Train and validate the model.\"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_dataloader, optimizer)\n",
        "        test_loss = validate(model, test_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eCVXzyquJNSk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 20  # Number of epochs to train\n",
        "train_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqBKMsVFpL6V",
        "outputId": "b57ccac9-cbe5-46eb-f7aa-f15ab6ccadf7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.3418, Test Loss: 0.2845\n",
            "Epoch 2/20, Train Loss: 0.3179, Test Loss: 0.2827\n",
            "Epoch 3/20, Train Loss: 0.3076, Test Loss: 0.2754\n",
            "Epoch 4/20, Train Loss: 0.2998, Test Loss: 0.2737\n",
            "Epoch 5/20, Train Loss: 0.2943, Test Loss: 0.2698\n",
            "Epoch 6/20, Train Loss: 0.2901, Test Loss: 0.2686\n",
            "Epoch 7/20, Train Loss: 0.2843, Test Loss: 0.2684\n",
            "Epoch 8/20, Train Loss: 0.2806, Test Loss: 0.2681\n",
            "Epoch 9/20, Train Loss: 0.2736, Test Loss: 0.2737\n",
            "Epoch 10/20, Train Loss: 0.2657, Test Loss: 0.2861\n",
            "Epoch 11/20, Train Loss: 0.2512, Test Loss: 0.2865\n",
            "Epoch 12/20, Train Loss: 0.2442, Test Loss: 0.2897\n",
            "Epoch 13/20, Train Loss: 0.2340, Test Loss: 0.3044\n",
            "Epoch 14/20, Train Loss: 0.2193, Test Loss: 0.3309\n",
            "Epoch 15/20, Train Loss: 0.2132, Test Loss: 0.3183\n",
            "Epoch 16/20, Train Loss: 0.2029, Test Loss: 0.3780\n",
            "Epoch 17/20, Train Loss: 0.2003, Test Loss: 0.3446\n",
            "Epoch 18/20, Train Loss: 0.1851, Test Loss: 0.3667\n",
            "Epoch 19/20, Train Loss: 0.1681, Test Loss: 0.3600\n",
            "Epoch 20/20, Train Loss: 0.1600, Test Loss: 0.3743\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/huggingface/peft\n",
        "!pip -q install --no-dependencies peft"
      ],
      "metadata": {
        "id": "eOP59w_uiGdu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f5a6f8-e0b8-48a4-c86d-60d32674a37a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "nfLqQEaQi2Y5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    r=4,\n",
        "    lora_dropout=0.15,\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "Aw_oZzOwi-rn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVU4ut_YstJK",
        "outputId": "3f970d6a-2799-49cd-a335-2fce5004c080",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): EsmForTokenClassification(\n",
              "      (esm): EsmModel(\n",
              "        (embeddings): EsmEmbeddings(\n",
              "          (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
              "          (dropout): Dropout(p=0.15, inplace=False)\n",
              "          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
              "        )\n",
              "        (encoder): EsmEncoder(\n",
              "          (layer): ModuleList(\n",
              "            (0-5): 6 x EsmLayer(\n",
              "              (attention): EsmAttention(\n",
              "                (self): EsmSelfAttention(\n",
              "                  (query): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.15, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=320, out_features=4, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=4, out_features=320, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (key): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.15, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=320, out_features=4, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=4, out_features=320, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (value): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.15, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=320, out_features=4, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=4, out_features=320, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                  (rotary_embeddings): RotaryEmbedding()\n",
              "                )\n",
              "                (output): EsmSelfOutput(\n",
              "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (dropout): Dropout(p=0.15, inplace=False)\n",
              "                )\n",
              "                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (intermediate): EsmIntermediate(\n",
              "                (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
              "              )\n",
              "              (output): EsmOutput(\n",
              "                (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                (dropout): Dropout(p=0.15, inplace=False)\n",
              "              )\n",
              "              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (contact_head): EsmContactPredictionHead(\n",
              "          (regression): Linear(in_features=120, out_features=1, bias=True)\n",
              "          (activation): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (classifier): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=320, out_features=1, bias=True)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYttG_Pqj8fb",
        "outputId": "57dd6711-ab0d-423f-fc6a-6e73e79734b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46401"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 20  # Number of epochs to train\n",
        "train_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)"
      ],
      "metadata": {
        "id": "T4YURzqqAE2f",
        "outputId": "2bdbb51f-bf22-409d-d4ce-f78e7f9d96c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.5323, Test Loss: 0.3209\n",
            "Epoch 2/20, Train Loss: 0.3232, Test Loss: 0.2884\n",
            "Epoch 3/20, Train Loss: 0.3218, Test Loss: 0.2860\n",
            "Epoch 4/20, Train Loss: 0.3197, Test Loss: 0.2852\n",
            "Epoch 5/20, Train Loss: 0.3158, Test Loss: 0.2852\n",
            "Epoch 6/20, Train Loss: 0.3183, Test Loss: 0.2845\n",
            "Epoch 7/20, Train Loss: 0.3184, Test Loss: 0.2839\n",
            "Epoch 8/20, Train Loss: 0.3171, Test Loss: 0.2823\n",
            "Epoch 9/20, Train Loss: 0.3155, Test Loss: 0.2776\n",
            "Epoch 10/20, Train Loss: 0.3076, Test Loss: 0.2751\n",
            "Epoch 11/20, Train Loss: 0.3012, Test Loss: 0.2762\n",
            "Epoch 12/20, Train Loss: 0.3050, Test Loss: 0.2808\n",
            "Epoch 13/20, Train Loss: 0.3021, Test Loss: 0.2722\n",
            "Epoch 14/20, Train Loss: 0.2976, Test Loss: 0.2719\n",
            "Epoch 15/20, Train Loss: 0.2939, Test Loss: 0.2721\n",
            "Epoch 16/20, Train Loss: 0.2932, Test Loss: 0.2714\n",
            "Epoch 17/20, Train Loss: 0.2893, Test Loss: 0.2746\n",
            "Epoch 18/20, Train Loss: 0.2892, Test Loss: 0.2818\n",
            "Epoch 19/20, Train Loss: 0.2889, Test Loss: 0.2708\n",
            "Epoch 20/20, Train Loss: 0.2872, Test Loss: 0.2771\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MhPCLzvlNIgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}