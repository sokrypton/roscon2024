{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/roscon2024/blob/main/extras/finetune_esm2_large.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**finefine esm2** (tutorial for wednesday)\n",
        "Special Thanks to Amelie Schreiber\n",
        "https://github.com/Amelie-Schreiber/esm2_loras"
      ],
      "metadata": {
        "id": "wqjacq79TckC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"esm2_t33_650M_UR50D\" # @param [\"esm2_t33_650M_UR50D\", \"esm2_t30_150M_UR50D\", \"esm2_t12_35M_UR50D\", \"esm2_t6_8M_UR50D\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k77krszTpJAn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtRKmskxgHfs"
      },
      "source": [
        "%%time\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import EsmForSequenceClassification, EsmForTokenClassification, AutoTokenizer\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = EsmForTokenClassification.from_pretrained(f\"facebook/{model_name}\",\n",
        "                                                  num_labels=1,\n",
        "                                                  hidden_dropout_prob=0.15)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Get DATA\n",
        "batch_size = 8 # @param {\"type\":\"integer\"}\n",
        "max_crop_len = 256 # @param {\"type\":\"integer\"}\n",
        "\n",
        "!wget -qnc https://github.com/sokrypton/roscon2024/raw/main/af2bind_data_0.pkl\n",
        "import pickle\n",
        "with open(\"af2bind_data_0.pkl\", \"rb\") as handle:\n",
        "  DATA = pickle.load(handle)\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "# Helper function to pad sequences\n",
        "def pad_sequence(seq, max_len, pad_value=0):\n",
        "    pad_size = max(0, max_len - len(seq))\n",
        "    return np.pad(seq, (0, pad_size), 'constant', constant_values=pad_value)[:max_len]\n",
        "\n",
        "class CustomProteinDataset(Dataset):\n",
        "    def __init__(self, inputs, attention_masks, outputs, masks, max_crop_len=128):\n",
        "        self.inputs = inputs\n",
        "        self.attention_masks = attention_masks\n",
        "        self.outputs = outputs\n",
        "        self.masks = masks\n",
        "        self.max_crop_len = max_crop_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.inputs[idx]\n",
        "        attention_mask = self.attention_masks[idx]\n",
        "        output = self.outputs[idx]\n",
        "        mask = self.masks[idx]\n",
        "\n",
        "        # Calculate the true length of the sequence (where attention_mask == 1)\n",
        "        true_len = int(np.sum(attention_mask))\n",
        "\n",
        "        # Determine the crop length (if the true length is less than max_crop_len, use true_len)\n",
        "        crop_len = min(self.max_crop_len, true_len)\n",
        "\n",
        "        # Randomly sample a crop starting index\n",
        "        if true_len > crop_len:\n",
        "            start_idx = np.random.randint(0, true_len - crop_len + 1)\n",
        "        else:\n",
        "            start_idx = 0\n",
        "\n",
        "        # Crop the sequences\n",
        "        input_ids = input_ids[start_idx:start_idx + crop_len]\n",
        "        attention_mask = attention_mask[start_idx:start_idx + crop_len].astype(np.float32)\n",
        "        output = output[start_idx:start_idx + crop_len].astype(np.float32)\n",
        "        mask = mask[start_idx:start_idx + crop_len].astype(np.float32)\n",
        "\n",
        "        # Pad the cropped sequences to max_crop_len\n",
        "        input_ids = pad_sequence(input_ids, self.max_crop_len)\n",
        "        attention_mask = pad_sequence(attention_mask, self.max_crop_len)\n",
        "        output = pad_sequence(output, self.max_crop_len)\n",
        "        mask = pad_sequence(mask, self.max_crop_len)\n",
        "\n",
        "        return torch.tensor(input_ids), torch.tensor(attention_mask), torch.tensor(output), torch.tensor(mask)\n",
        "\n",
        "# Create DataLoaders\n",
        "dataloaders = []\n",
        "for v in range(3):  # train/test/validation\n",
        "    dataset = CustomProteinDataset(DATA[\"inputs\"][v], DATA[\"attention_masks\"][v],\n",
        "                                   DATA[\"outputs\"][v], DATA[\"masks\"][v],\n",
        "                                   max_crop_len=max_crop_len)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=(v == 0))\n",
        "    dataloaders.append(dataloader)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fUQAZkhEIRzU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ##Training Code\n",
        "\n",
        "def compute_loss(logits, labels, mask):\n",
        "  \"\"\"Compute masked loss.\"\"\"\n",
        "  loss = nn.BCEWithLogitsLoss(reduction='none')(logits, labels)\n",
        "  masked_loss = loss * mask\n",
        "  mean_loss = masked_loss.sum() / mask.sum()\n",
        "  return mean_loss\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer):\n",
        "  \"\"\"Train the model for one epoch.\"\"\"\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for batch in dataloader:\n",
        "    inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n",
        "    logits = outputs.logits.squeeze(-1)\n",
        "\n",
        "    # Compute loss\n",
        "    mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()\n",
        "    mean_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += mean_loss.item()\n",
        "\n",
        "  average_loss = total_loss / len(dataloader)\n",
        "  return average_loss\n",
        "\n",
        "def validate(model, dataloader):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in dataloader:\n",
        "        inputs_batch, attention_masks_batch, true_labels_batch, mask_batch = [x.to(DEVICE) for x in batch]\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=inputs_batch, attention_mask=attention_masks_batch)\n",
        "        logits = outputs.logits.squeeze(-1)\n",
        "\n",
        "        # Compute loss\n",
        "        mean_loss = compute_loss(logits, true_labels_batch, mask_batch)\n",
        "\n",
        "        total_loss += mean_loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "def train_model(model, train_dataloader, test_dataloader, num_epochs, optimizer):\n",
        "    \"\"\"Train and validate the model.\"\"\"\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_dataloader, optimizer)\n",
        "        test_loss = validate(model, test_dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eCVXzyquJNSk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)\n",
        "trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1_WrbxVPKZQ",
        "outputId": "52bb2930-e581-438e-fe3f-32838361d972"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "650715542"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/huggingface/peft\n",
        "!pip -q install --no-dependencies peft"
      ],
      "metadata": {
        "id": "eOP59w_uiGdu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType"
      ],
      "metadata": {
        "id": "nfLqQEaQi2Y5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    r=4,\n",
        "    lora_dropout=0.15,\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "Aw_oZzOwi-rn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVU4ut_YstJK",
        "outputId": "3f970d6a-2799-49cd-a335-2fce5004c080",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): EsmForTokenClassification(\n",
              "      (esm): EsmModel(\n",
              "        (embeddings): EsmEmbeddings(\n",
              "          (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
              "          (dropout): Dropout(p=0.15, inplace=False)\n",
              "          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
              "        )\n",
              "        (encoder): EsmEncoder(\n",
              "          (layer): ModuleList(\n",
              "            (0-5): 6 x EsmLayer(\n",
              "              (attention): EsmAttention(\n",
              "                (self): EsmSelfAttention(\n",
              "                  (query): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.15, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=320, out_features=4, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=4, out_features=320, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (key): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.15, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=320, out_features=4, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=4, out_features=320, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (value): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.15, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=320, out_features=4, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=4, out_features=320, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                    (lora_magnitude_vector): ModuleDict()\n",
              "                  )\n",
              "                  (dropout): Dropout(p=0.0, inplace=False)\n",
              "                  (rotary_embeddings): RotaryEmbedding()\n",
              "                )\n",
              "                (output): EsmSelfOutput(\n",
              "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (dropout): Dropout(p=0.15, inplace=False)\n",
              "                )\n",
              "                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              )\n",
              "              (intermediate): EsmIntermediate(\n",
              "                (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
              "              )\n",
              "              (output): EsmOutput(\n",
              "                (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                (dropout): Dropout(p=0.15, inplace=False)\n",
              "              )\n",
              "              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (contact_head): EsmContactPredictionHead(\n",
              "          (regression): Linear(in_features=120, out_features=1, bias=True)\n",
              "          (activation): Sigmoid()\n",
              "        )\n",
              "      )\n",
              "      (dropout): Dropout(p=0.15, inplace=False)\n",
              "      (classifier): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=320, out_features=1, bias=True)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=320, out_features=1, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = lambda x: sum(p.numel() for p in x.parameters() if p.requires_grad)\n",
        "trainable_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYttG_Pqj8fb",
        "outputId": "538acbf0-9a04-40a3-a9e5-da902d8698fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1015041"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "num_epochs = 20  # Number of epochs to train\n",
        "train_model(model, dataloaders[0], dataloaders[1], num_epochs, optimizer)"
      ],
      "metadata": {
        "id": "T4YURzqqAE2f",
        "outputId": "8445a87c-f9d2-40f5-9a49-8ce6c21da9eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Train Loss: 0.3604, Test Loss: 0.4606\n",
            "Epoch 2/20, Train Loss: 0.3342, Test Loss: 0.4910\n",
            "Epoch 3/20, Train Loss: 0.3292, Test Loss: 0.4799\n",
            "Epoch 4/20, Train Loss: 0.3253, Test Loss: 0.4702\n",
            "Epoch 5/20, Train Loss: 0.3218, Test Loss: 0.4634\n",
            "Epoch 6/20, Train Loss: 0.3205, Test Loss: 0.4656\n",
            "Epoch 7/20, Train Loss: 0.3197, Test Loss: 0.4324\n",
            "Epoch 8/20, Train Loss: 0.3121, Test Loss: 0.4353\n",
            "Epoch 9/20, Train Loss: 0.3114, Test Loss: 0.4140\n",
            "Epoch 10/20, Train Loss: 0.3056, Test Loss: 0.4060\n",
            "Epoch 11/20, Train Loss: 0.3035, Test Loss: 0.3634\n",
            "Epoch 12/20, Train Loss: 0.3007, Test Loss: 0.3708\n",
            "Epoch 13/20, Train Loss: 0.2957, Test Loss: 0.3445\n",
            "Epoch 14/20, Train Loss: 0.2874, Test Loss: 0.2831\n",
            "Epoch 15/20, Train Loss: 0.2748, Test Loss: 0.2445\n",
            "Epoch 16/20, Train Loss: 0.2558, Test Loss: 0.2493\n",
            "Epoch 17/20, Train Loss: 0.2386, Test Loss: 0.2215\n",
            "Epoch 18/20, Train Loss: 0.2219, Test Loss: 0.2087\n",
            "Epoch 19/20, Train Loss: 0.2108, Test Loss: 0.2228\n",
            "Epoch 20/20, Train Loss: 0.1973, Test Loss: 0.2201\n",
            "Training complete.\n"
          ]
        }
      ]
    }
  ]
}